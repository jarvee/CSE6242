A
1.RandomForest -P 100 -I 100 -num-slots 1 -K 0 -M 1.0 -V 0.001 -S 1
Time taken to build model: 0.22 seconds
Overall accuracy: 86.6769 %
Confusion Matrix:

   a   b   <-- classified as
 316  41 |   a = 0
  46 250 |   b = 1

2.Logistic -R 1.0E-8 -M -1 -num-decimal-places 4
Time taken to build model: 0.11 seconds
Overall accuracy: 86.3706 %
Confusion Matrix:


   a   b   <-- classified as
 302  55 |   a = 0
  34 262 |   b = 1

3.MultilayerPerceptron -L 0.3 -M 0.2 -N 500 -V 0 -S 0 -E 20 -H a
Time taken to build model: 4.21 seconds
Overall accuracy: 83.3078 %
Confusion Matrix:

   a   b   <-- classified as
 298  59 |   a = 0
  50 246 |   b = 1

4.SGD -F 0 -L 0.01 -R 1.0E-4 -E 500 -C 0.001 -S 1
Time taken to build model: 0.13 seconds
Overall accuracy: 85.9112 %
Confusion Matrix:

   a   b   <-- classified as
 286  71 |   a = 0
  21 275 |   b = 1


5.DecisionTable -X 1 -S "weka.attributeSelection.BestFirst -D 1 -N 5"
Time taken to build model: 0.17 seconds
Overall accuracy: 85.6049 %
Confusion Matrix:

   a   b   <-- classified as
 305  52 |   a = 0
  42 254 |   b = 1

B.
1a.
I modified the BagsizePercent to 80.Runtime increased to 0.1 seconds.
Percent accuracy increased to 87.1363 %.
Confusion Matrix changed to:
   a   b   <-- classified as
 317  40 |   a = 0
  44 252 |   b = 1It was because BagsizePercent as a percentage of the raw training dataset, when we reduce the value to 80, it will take 80% to crate a new random dataset sample, it will have a different composition and use less time.

1b.
I modified the Maxits to 1.Runtime increased to 0.01 seconds.
Percent accuracy increased to 81.317 %.
Confusion Matrix changed to:
   a   b   <-- classified as
 299  58 |   a = 0
  64 232 |   b = 1
It was because there maxits = -1 represent continued till the feature-class weights are fully optimized. When we change to 1, the classifier when trained on 1 cross-validation round only tend to be unstable.

1c.
I modified the momentum to 0.4.Runtime increased to 4.04 seconds.
Confusion Matrix changed to:
   a   b   <-- classified as
 305  52 |   a = 0
  58 238 |   b = 1It was because momentum is a multiplier to the learning rate, once the the error rate is decreasing, the momentum term accelerates the progress.

1d.
I modified the learning rate to 0.0005.Runtime increased to 0.04 seconds.
Percent accuracy increased to 86.3706 %.
Confusion Matrix changed to:
   a   b   <-- classified as
 286  71 |   a = 0
  18 278 |   b = 1
It was because when the learning rate is sufficiently small, this algorithm achieves linear.

1e.
I modified the crossVal to 10.Runtime increased to 0.35 seconds.
Percent accuracy increased to 86.3706 %.
Confusion Matrix changed to:
   a   b   <-- classified as
 304  53 |   a = 0
  36 260 |   b = 1
It was because crossval represent sets the number of folds for cross validation. Increase the crossval will get more accuracy stable trained dataset but take longer time.

2. The accuracy of my implementation was 85.76%, compared to wekaâ€™s 86.6769 %. It was because the random forest classifier in Weka has a numIterations of 100 and 10 folds Cross-validation, It will control the convergency of trained dataset. Also, the percentage of split is 66%, which is different from our random forest so it will improve the speed and get more accurate result.

3.In terms of accuracy and running time, the best performing approach was random forest classifier, because the random forest takes the highest accuracy of 87.1363% and acceptable of running time 0.1 seconds. Also, the random forest can control the maxdepth and bagsize so that we can control the running speed and improve the accuracy of the learner. Random forest can avoid overfitting in some way.
